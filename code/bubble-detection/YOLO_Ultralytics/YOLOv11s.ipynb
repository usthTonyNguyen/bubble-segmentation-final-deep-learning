{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a2473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:11.860677Z",
     "iopub.status.busy": "2025-10-06T05:37:11.860356Z",
     "iopub.status.idle": "2025-10-06T05:37:11.871064Z",
     "shell.execute_reply": "2025-10-06T05:37:11.870388Z",
     "shell.execute_reply.started": "2025-10-06T05:37:11.860642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Import Required Libraries ---\n",
    "\n",
    "# File and data handling\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing and visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm  \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df32c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = os.getcwd()\n",
    "\n",
    "if not NOTEBOOK_DIR.endswith(\"YOLO_Ultralytics\"):\n",
    "    raise ValueError(\"Please set the working directory to 'YOLO_Ultralytics' folder. Currently it is set to: \" + NOTEBOOK_DIR)\n",
    "\n",
    "BASE_DIR = os.path.join(NOTEBOOK_DIR, \"..\", \"..\", \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd15b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T05:37:11.871982Z",
     "iopub.status.busy": "2025-10-06T05:37:11.871795Z",
     "iopub.status.idle": "2025-10-06T05:37:11.889870Z",
     "shell.execute_reply": "2025-10-06T05:37:11.889117Z",
     "shell.execute_reply.started": "2025-10-06T05:37:11.871968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 1. Configuration Section ---\n",
    "\n",
    "# Paths to your data and model files\n",
    "JSON_DIR = os.path.join(BASE_DIR, 'data', 'MangaSegmentation/jsons_processed') \n",
    "IMAGE_ROOT_DIR = os.path.join(BASE_DIR, 'data', 'Manga109_released_2023_12_07','images')  \n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'data', 'YOLO_data')\n",
    "\n",
    "EPOCHS = 1  # Number of training epochs: Used 20, set to 1 for easy testing\n",
    "IMAGE_SIZE = 160  # Image size for training: Used 640, set to 160 for easy testing\n",
    "BATCH_SIZE = 2  # Batch size for training: Used 8 \n",
    "yaml_path = Path(DATASET_DIR) / 'dataset.yaml'\n",
    "\n",
    "# Validate paths\n",
    "print(\"\\nValidating directories...\")\n",
    "for path in [JSON_DIR, IMAGE_ROOT_DIR]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Directory not found: {path}\")\n",
    "    else:\n",
    "        print(f\"Found directory: {path}\")\n",
    "        # List some contents\n",
    "        contents = os.listdir(path)[:5]\n",
    "        print(f\"Sample contents: {contents}\")\n",
    "\n",
    "# Create dataset directories\n",
    "print(\"\\nCreating dataset directories...\")\n",
    "for split in ['train', 'val']:\n",
    "    for subdir in ['images', 'labels']:  \n",
    "        dir_path = os.path.join(DATASET_DIR, f'{subdir}/{split}')\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"Created: {dir_path}\")\n",
    "\n",
    "# Set category information directly\n",
    "TARGET_CATEGORY_ID = 5  # Fixed category ID for balloon\n",
    "TARGET_CATEGORY_NAME = \"balloon\"  # Fixed category name\n",
    "\n",
    "print(\"Target Category Configuration:\")\n",
    "print(f\"Category ID: {TARGET_CATEGORY_ID}\")\n",
    "print(f\"Category Name: {TARGET_CATEGORY_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  Model Training \n",
    "# ===================================================================\n",
    "print(\"\\n--- 5. Initializing and Training YOLOv11 Model ---\")\n",
    "\n",
    "PROJECT_DIR = os.path.join(BASE_DIR, 'models', 'bubble-detection', 'YOLOv11s_Training_Results')\n",
    "\n",
    "# 1. Load a pretrained YOLOv11 segmentation model\n",
    "model = YOLO('yolo11s-seg.pt')\n",
    "\n",
    "# 2. Train the model with essential parameters\n",
    "print(\"\\nStarting model training...\")\n",
    "results = model.train(\n",
    "    data=str(yaml_path),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    project=PROJECT_DIR,\n",
    "    name='balloon_segmentation_run1',\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")\n",
    "print(f\"All results, logs, and plots have been saved to: {model.trainer.save_dir}\")\n",
    "print(f\"The best performing model is saved as: {model.trainer.best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7903cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Automatic and Comprehensive Evaluation\n",
    "# ===================================================================\n",
    "print(\"\\n--- 6. Evaluating Final Model Performance ---\")\n",
    "\n",
    "# 1. Load the best model that was saved during training\n",
    "path_to_best_model = model.trainer.best\n",
    "if not os.path.exists(path_to_best_model):\n",
    "    raise FileNotFoundError(f\"Could not find the best model at: {path_to_best_model}\")\n",
    "\n",
    "print(f\"Loading best model from: {path_to_best_model}\")\n",
    "best_model = YOLO(path_to_best_model)\n",
    "\n",
    "# 2. Run validation on the 'val' split to get the metrics object\n",
    "print(\"\\nRunning final validation on the test set...\")\n",
    "metrics = best_model.val(\n",
    "    split='val',\n",
    "    project=PROJECT_DIR,\n",
    "    name='balloon_segmentation_run1',\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "# 3. Automatically discover, group, and print all available metrics\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"--- Final Comprehensive Evaluation Report (All Metrics) ---\")\n",
    "print(\"#\"*60)\n",
    "print(f\"\\nValidation results saved to: {metrics.save_dir}\\n\")\n",
    "\n",
    "# Dictionaries to hold the grouped metrics\n",
    "box_metrics = {}\n",
    "mask_metrics = {}\n",
    "other_metrics = {}\n",
    "\n",
    "# Iterate through all key-value pairs in the results dictionary\n",
    "for key, value in metrics.results_dict.items():\n",
    "    # Clean the key by removing the 'metrics/' prefix\n",
    "    clean_key = key.replace('metrics/', '').strip()\n",
    "    \n",
    "    # Sort keys into their respective groups\n",
    "    if '(B)' in clean_key:\n",
    "        final_key = clean_key.replace('(B)', '').strip()\n",
    "        box_metrics[final_key] = value\n",
    "    elif '(M)' in clean_key:\n",
    "        final_key = clean_key.replace('(M)', '').strip()\n",
    "        mask_metrics[final_key] = value\n",
    "    else:\n",
    "        other_metrics[clean_key] = value\n",
    "\n",
    "# --- Function to print a dictionary of metrics neatly ---\n",
    "def print_metric_group(title, metric_dict):\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    if not metric_dict:\n",
    "        print(\"     (No metrics found for this group)\")\n",
    "        return\n",
    "    # Sort keys for consistent ordering\n",
    "    for key in sorted(metric_dict.keys()):\n",
    "        value = metric_dict[key]\n",
    "        # Use a fixed width for the key for nice alignment\n",
    "        print(f\"     - {key:<15}: {value:.4f}\")\n",
    "\n",
    "# --- Print each group of metrics ---\n",
    "print_metric_group(\"Bounding Box Detection Performance\", box_metrics)\n",
    "print_metric_group(\"Instance Segmentation Performance\", mask_metrics)\n",
    "print_metric_group(\"Other Metrics (e.g., Losses)\", other_metrics)\n",
    "\n",
    "print(\"\\n\" + \"#\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf93863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Export All Validation Predictions\n",
    "# ===================================================================\n",
    "print(\"\\n--- 7. Exporting All Predictions from the Validation Set ---\")\n",
    "\n",
    "# 1. Load the best model again\n",
    "# The path is available from the previous cell: model.trainer.best\n",
    "path_to_best_model = model.trainer.best\n",
    "if not os.path.exists(path_to_best_model):\n",
    "    raise FileNotFoundError(f\"Could not find the best model at: {path_to_best_model}\")\n",
    "\n",
    "print(f\"Loading best model from: {path_to_best_model}\")\n",
    "best_model = YOLO(path_to_best_model)\n",
    "\n",
    "# 2. Define the path to the validation images\n",
    "validation_images_path = os.path.join(DATASET_DIR, 'images/val')\n",
    "image_files = [os.path.join(validation_images_path, f) for f in os.listdir(validation_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "print(f\"Found {len(image_files)} images to predict in: {validation_images_path}\")\n",
    "\n",
    "# 3. Loop through each image and predict one by one to avoid OOM\n",
    "output_project = PROJECT_DIR\n",
    "output_name = 'all_validation_predictions'\n",
    "\n",
    "print(f\"Starting prediction... Results will be saved in '{output_project}/{output_name}'\")\n",
    "for image_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "    # Predict on a SINGLE image.\n",
    "    best_model.predict(\n",
    "        source=image_path,\n",
    "        project=output_project,\n",
    "        name=output_name,\n",
    "        exist_ok=True,  # This is important to ensure all results save to the SAME folder\n",
    "        save=True       # Save the image with predictions\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Prediction Export Finished ---\")\n",
    "print(f\"All prediction images have been saved to the '{output_project}/{output_name}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Advanced Evaluation: Calculating Boundary-AP\n",
    "# ===================================================================\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "print(\"\\n--- 8. Advanced: Calculating Boundary-AP ---\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_gt_polygons(label_path, img_height, img_width):\n",
    "    \"\"\"Loads ground-truth polygons from a YOLO .txt file.\"\"\"\n",
    "    polygons = []\n",
    "    if not os.path.exists(label_path):\n",
    "        return polygons\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            # Denormalize polygon\n",
    "            poly = np.array([float(p) for p in parts[1:]]).reshape(-1, 2)\n",
    "            poly[:, 0] *= img_width\n",
    "            poly[:, 1] *= img_height\n",
    "            polygons.append(poly.astype(np.int32))\n",
    "    return polygons\n",
    "\n",
    "def polygons_to_mask(polygons, height, width):\n",
    "    \"\"\"Converts a list of polygons to a binary mask.\"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, pts=polygons, color=255)\n",
    "    return mask\n",
    "\n",
    "def calculate_area_iou(gt_polygons, pred_polygon, height, width):\n",
    "    \"\"\"Calculates Area IoU between a predicted polygon and all GT polygons.\"\"\"\n",
    "    pred_mask = polygons_to_mask([pred_polygon], height, width)\n",
    "    gt_mask = polygons_to_mask(gt_polygons, height, width)\n",
    "    \n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def calculate_boundary_iou(gt_poly, pred_poly, height, width, thickness=2):\n",
    "    \"\"\"Calculates Boundary IoU for a single pair of polygons.\"\"\"\n",
    "    gt_boundary_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    pred_boundary_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    cv2.polylines(gt_boundary_mask, [gt_poly], isClosed=True, color=255, thickness=thickness)\n",
    "    cv2.polylines(pred_boundary_mask, [pred_poly], isClosed=True, color=255, thickness=thickness)\n",
    "    \n",
    "    intersection = np.logical_and(gt_boundary_mask, pred_boundary_mask).sum()\n",
    "    union = np.logical_or(gt_boundary_mask, pred_boundary_mask).sum()\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# --- Main Evaluation Logic ---\n",
    "\n",
    "# 1. Load the best model\n",
    "path_to_best_model = model.trainer.best\n",
    "best_model = YOLO(path_to_best_model)\n",
    "\n",
    "# 2. Get paths for validation images and labels\n",
    "val_img_dir = os.path.join(DATASET_DIR, 'images/val')\n",
    "val_label_dir = os.path.join(DATASET_DIR, 'labels/val')\n",
    "val_image_files = [os.path.join(val_img_dir, f) for f in os.listdir(val_img_dir)]\n",
    "\n",
    "# 3. Run prediction to get results object\n",
    "print(f\"Running predictions on {len(val_image_files)} validation images to get polygon data...\")\n",
    "results = best_model.predict(source=val_img_dir, stream=True) # Use stream for memory efficiency\n",
    "\n",
    "all_predictions = [] # Store tuples of (confidence, is_tp)\n",
    "\n",
    "for result in tqdm(results, total=len(val_image_files), desc=\"Evaluating Boundary Quality\"):\n",
    "    img_path = Path(result.path)\n",
    "    h, w = result.orig_shape\n",
    "    \n",
    "    # Load corresponding ground truth\n",
    "    label_path = os.path.join(val_label_dir, img_path.stem + '.txt')\n",
    "    gt_polygons = load_gt_polygons(label_path, h, w)\n",
    "    \n",
    "    # Get predictions for this image\n",
    "    pred_polygons_normalized = result.masks.xyn if result.masks else []\n",
    "    pred_confs = result.boxes.conf if result.boxes else []\n",
    "\n",
    "    # Keep track of which GT polygons have been \"matched\"\n",
    "    gt_matched = [False] * len(gt_polygons)\n",
    "    \n",
    "    # Sort predictions by confidence\n",
    "    if len(pred_polygons_normalized) > 0:\n",
    "        sorted_preds = sorted(zip(pred_confs, pred_polygons_normalized), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for conf, poly_norm in sorted_preds:\n",
    "            # Denormalize predicted polygon\n",
    "            pred_poly = (poly_norm * np.array([w, h])).astype(np.int32)\n",
    "            \n",
    "            best_match_idx = -1\n",
    "            best_area_iou = 0.5 # Matching threshold\n",
    "\n",
    "            # Find the best GT match for this prediction using Area IoU\n",
    "            for i, gt_poly in enumerate(gt_polygons):\n",
    "                if not gt_matched[i]:\n",
    "                    area_iou = calculate_area_iou([gt_poly], pred_poly, h, w)\n",
    "                    if area_iou > best_area_iou:\n",
    "                        best_area_iou = area_iou\n",
    "                        best_match_idx = i\n",
    "\n",
    "            is_tp = False\n",
    "            if best_match_idx != -1:\n",
    "                # We found a match, now score it with Boundary IoU\n",
    "                gt_matched[best_match_idx] = True\n",
    "                matched_gt_poly = gt_polygons[best_match_idx]\n",
    "                \n",
    "                b_iou = calculate_boundary_iou(matched_gt_poly, pred_poly, h, w)\n",
    "                \n",
    "                # If Boundary IoU is high enough, it's a True Positive\n",
    "                if b_iou > 0.75: # Boundary IoU threshold\n",
    "                    is_tp = True\n",
    "            \n",
    "            all_predictions.append({'confidence': float(conf), 'is_tp': is_tp})\n",
    "\n",
    "# 4. Calculate Boundary-AP\n",
    "if not all_predictions:\n",
    "    print(\"\\nNo predictions were made. Boundary-AP is 0.\")\n",
    "else:\n",
    "    all_predictions.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    y_true = [int(p['is_tp']) for p in all_predictions]\n",
    "    y_scores = [p['confidence'] for p in all_predictions]\n",
    "    \n",
    "    # Also need to account for False Negatives\n",
    "    num_gt_total = sum(len(load_gt_polygons(os.path.join(val_label_dir, Path(f).stem + '.txt'), 1, 1)) for f in val_image_files)\n",
    "    num_tp = sum(y_true)\n",
    "    num_fn = num_gt_total - num_tp\n",
    "    \n",
    "    # Append FNs to the results\n",
    "    y_true.extend([1] * num_fn)\n",
    "    y_scores.extend([0] * num_fn)\n",
    "    \n",
    "    boundary_ap = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"--- Custom Boundary-AP Evaluation Report ---\")\n",
    "    print(\"#\"*60)\n",
    "    print(f\"\\n     - Total Ground-Truth Balloons : {num_gt_total}\")\n",
    "    print(f\"     - Total Predictions Evaluated : {len(all_predictions)}\")\n",
    "    print(f\"     - True Positives (Boundary IoU > 0.75): {num_tp}\")\n",
    "    print(f\"     - Boundary-AP                  : {boundary_ap:.4f}\")\n",
    "    print(\"\\n\" + \"#\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8407158,
     "sourceId": 13266819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8408218,
     "sourceId": 13268335,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
